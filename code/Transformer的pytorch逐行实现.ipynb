{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer模型结构\n",
    "![image.png](../add_pic/transformer结构.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from labml_helpers.module import Module\n",
    "from labml_nn.utils import clone_module_list\n",
    "from typing import Optional, List\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(Module):\n",
    "    def __init__(self, d_model: int, d_ff: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 activation=nn.ReLU(),\n",
    "                 is_gated: bool = False,\n",
    "                 bias1: bool = True,\n",
    "                 bias2: bool = True,\n",
    "                 bias_gate: bool = True):\n",
    "        super().__init__()\n",
    "        # 初始化第一层线性变换，输入维度为 d_model，输出维度为 d_ff\n",
    "        self.layer1 = nn.Linear(d_model, d_ff, bias=bias1)\n",
    "        # 初始化第二层线性变换，输入维度为 d_ff，输出维度为 d_model\n",
    "        self.layer2 = nn.Linear(d_ff, d_model, bias=bias2)\n",
    "        # 初始化 dropout 层，用于在训练过程中随机关闭部分神经元\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 激活函数，用于引入非线性\n",
    "        self.activation = activation\n",
    "        # 是否启用门控机制\n",
    "        self.is_gated = is_gated\n",
    "        if is_gated:\n",
    "            # 如果启用了门控，初始化另一层线性变换，用于门控\n",
    "            self.linear_v = nn.Linear(d_model, d_ff, bias=bias_gate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 应用第一层线性变换和激活函数\n",
    "        g = self.activation(self.layer1(x))\n",
    "\n",
    "        # 如果启用了门控机制\n",
    "        if self.is_gated:\n",
    "            # 应用门控操作\n",
    "            x = g * self.linear_v(x)\n",
    "        else:\n",
    "            x = g\n",
    "\n",
    "        # 应用 dropout\n",
    "        x = self.dropout(x)\n",
    "        # 应用第二层线性变换\n",
    "        return self.layer2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../add_pic/transformer多头注意力机制1.png)\n",
    "![image.png](../add_pic/transformer多头注意力机制2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareForMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, d_k: int, bias: bool):\n",
    "        super().__init__()\n",
    "        # 初始化线性变换层，用于将输入转换为多头注意力所需的形状\n",
    "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
    "        # 注意力头的数量\n",
    "        self.heads = heads\n",
    "        # 每个头中向量的维度\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 获取输入的初始形状，用于之后的变形操作\n",
    "        head_shape = x.shape[:-1]\n",
    "\n",
    "        # 应用线性变换\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # 将最后一个维度分割成多个头，并为每个头分配 d_k 维度\n",
    "        x = x.view(*head_shape, self.heads, self.d_k)\n",
    "        # 返回的形状是 [seq_len, batch_size, heads, d_k] 或 [batch_size, heads, d_k]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1, bias: bool = True):\n",
    "        super().__init__()\n",
    "        # 计算每个注意力头的维度\n",
    "        self.d_k = d_model // heads\n",
    "        # 注意力头的数量\n",
    "        self.heads = heads\n",
    "\n",
    "        # 分别为 query, key 和 value 初始化线性层\n",
    "        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n",
    "        self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=True)\n",
    "\n",
    "        # 初始化 softmax 层，用于注意力权重的计算\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # 输出层，将多头注意力的输出合并并作为一个张量\n",
    "        self.output = nn.Linear(d_model, d_model)\n",
    "        # Dropout 层\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # 注意力计算的缩放因子\n",
    "        self.scale = 1 / math.sqrt(self.d_k)\n",
    "        # 存储注意力权重，用于调试或可视化\n",
    "        self.attn = None\n",
    "\n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        # 计算 query 和 key 的点积，用于注意力权重\n",
    "        return torch.einsum('ibhd,jbhd->ijbh', query, key)\n",
    "\n",
    "    def prepare_mask(self, mask: torch.Tensor, query_shape: List[int], key_shape: List[int]):\n",
    "        \"\"\"\n",
    "        调整 mask 为 (seq_len_q, seq_len_k, batch, 1)，\n",
    "        其中 query_shape 为 (seq_len_q, batch, d_model)。\n",
    "        \"\"\"\n",
    "        # mask 已经是 3 维\n",
    "        if mask.dim() == 3:\n",
    "            # 如果 mask 的第一维等于 batch，则说明输入 mask 原本形状是 (batch, seq_len, seq_len)\n",
    "            if mask.size(0) == query_shape[1]:\n",
    "                # 转换为 (seq_len, seq_len, batch)\n",
    "                mask = mask.permute(1, 2, 0)\n",
    "            # 如果 mask 的第二维为 1（例如 src_mask 原始形状为 (seq_len, 1, batch)），则扩展该维度\n",
    "            elif mask.size(1) == 1:\n",
    "                mask = mask.expand(-1, query_shape[0], -1)\n",
    "            # 否则，认为 mask 已经是 (seq_len, seq_len, batch)（例如 tgt_mask 经过转置后）\n",
    "            mask = mask.unsqueeze(-1)  # 变为 (seq_len, seq_len, batch, 1)\n",
    "        elif mask.dim() == 2:\n",
    "            mask = mask.unsqueeze(0).unsqueeze(-1)  # (1, seq_len, seq_len, 1)\n",
    "            mask = mask.expand(query_shape[0], -1, -1, -1)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported mask dimension: expected 2 or 3, got {}\".format(mask.dim()))\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def forward(self, *,\n",
    "                query: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None):\n",
    "        # 获取序列长度和批次大小\n",
    "        seq_len, batch_size, _ = query.shape\n",
    "\n",
    "        # 如果提供掩码，则进行调整\n",
    "        if mask is not None:\n",
    "            mask = self.prepare_mask(mask, query.shape, key.shape)\n",
    "\n",
    "        # 准备 query, key 和 value\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        # 计算注意力得分\n",
    "        scores = self.get_scores(query, key)\n",
    "        # 应用缩放因子\n",
    "        scores *= self.scale\n",
    "\n",
    "        # 如果有掩码，应用掩码\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # 应用 softmax 得到注意力权重\n",
    "        attn = self.softmax(scores)\n",
    "        # 应用 dropout\n",
    "        attn = self.dropout(attn)\n",
    "        # 根据注意力权重组合 value\n",
    "        x = torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n",
    "\n",
    "        # 存储注意力权重\n",
    "        self.attn = attn.detach()\n",
    "\n",
    "        # 合并多头注意力的输出\n",
    "        x = x.reshape(seq_len, batch_size, -1)\n",
    "        # 应用输出层\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../add_pic/transformer位置编码和嵌入.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(d_model: int, max_len: int = 5000):\n",
    "    # 初始化一个全零的位置信息矩阵，形状为 [max_len, d_model]\n",
    "    encodings = torch.zeros(max_len, d_model)\n",
    "    # 创建一个位置索引，形状为 [max_len, 1]\n",
    "    position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "    # 创建一个序列，用于在正弦和余弦函数中的分母，形状为 [d_model/2]\n",
    "    two_i = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
    "    # 计算分母项\n",
    "    div_term = torch.exp(two_i * -(math.log(10000.0) / d_model))\n",
    "\n",
    "    # 填充位置信息的偶数部分 (2i) 使用正弦函数\n",
    "    encodings[:, 0::2] = torch.sin(position * div_term)\n",
    "    # 填充位置信息的奇数部分 (2i+1) 使用余弦函数\n",
    "    encodings[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    # 增加一个批次维度，并禁止对位置编码本身计算梯度\n",
    "    encodings = encodings.unsqueeze(0).requires_grad_(False)\n",
    "    return encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout_prob: float, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # 初始化 dropout 层\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # 创建位置信息并注册为 buffer（不作为可训练参数）\n",
    "        self.register_buffer('positional_encodings',\n",
    "                             get_positional_encoding(d_model, max_len),\n",
    "                             persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 从 buffer 中取出与输入长度相同的位置信息\n",
    "        pe = self.positional_encodings[:x.shape[0]].detach().requires_grad_(False)\n",
    "        # 将位置编码添加到输入中\n",
    "        x = x + pe\n",
    "        # 应用 dropout\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsWithPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, n_vocab: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Embedding(n_vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "        # 注册位置编码，形状为 (1, max_len, d_model)\n",
    "        self.register_buffer('positional_encodings',\n",
    "                             get_positional_encoding(d_model, max_len),\n",
    "                             persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 假设 x 的形状是 (batch_size, seq_len)\n",
    "        # 取出对应序列长度的部分，形状变为 (1, seq_len, d_model)\n",
    "        pe = self.positional_encodings[:, :x.size(1)]\n",
    "        # 进行嵌入和缩放，再加上位置编码（利用广播机制，pe 会自动扩展到 (batch_size, seq_len, d_model)）\n",
    "        return self.linear(x) * math.sqrt(self.d_model) + pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsWithLearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, n_vocab: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # 创建一个嵌入层\n",
    "        self.linear = nn.Embedding(n_vocab, d_model)\n",
    "        # 存储模型维度\n",
    "        self.d_model = d_model\n",
    "        # 初始化一个可学习的位置信息\n",
    "        self.positional_encodings = nn.Parameter(\n",
    "            torch.zeros(max_len, 1, d_model),\n",
    "            requires_grad=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 获取可学习的位置信息\n",
    "        pe = self.positional_encodings[:x.shape[0]]\n",
    "        # 将嵌入向量以 sqrt(d_model) 进行缩放，并加上位置编码\n",
    "        return self.linear(x) * math.sqrt(self.d_model) + pe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../add_pic/Transformer层.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                 d_model: int,\n",
    "                 self_attn: MultiHeadAttention,\n",
    "                 src_attn: MultiHeadAttention = None,\n",
    "                 feed_forward: FeedForward,\n",
    "                 dropout_prob: float):\n",
    "        super().__init__()\n",
    "        # 设置模型的维度\n",
    "        self.size = d_model\n",
    "        # 自注意力机制\n",
    "        self.self_attn = self_attn\n",
    "        # 源注意力机制（解码器用，用于关注编码器输出），可为空\n",
    "        self.src_attn = src_attn\n",
    "        # 前馈网络\n",
    "        self.feed_forward = feed_forward\n",
    "        # Dropout 层\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # 对自注意力的输出进行层归一化\n",
    "        self.norm_self_attn = nn.LayerNorm([d_model])\n",
    "        # 如果存在源注意力，则对其输出进行层归一化\n",
    "        if self.src_attn is not None:\n",
    "            self.norm_src_attn = nn.LayerNorm([d_model])\n",
    "        # 对前馈网络的输出进行层归一化\n",
    "        self.norm_ff = nn.LayerNorm([d_model])\n",
    "\n",
    "        # 用于保存前馈网络的输入（若需调试或其他用途）\n",
    "        self.is_save_ff_input = False\n",
    "\n",
    "    def forward(self, *,\n",
    "                x: torch.Tensor,\n",
    "                mask: torch.Tensor,\n",
    "                src: torch.Tensor = None,\n",
    "                src_mask: torch.Tensor = None):\n",
    "        # 对输入进行层归一化后再进行自注意力计算\n",
    "        z = self.norm_self_attn(x)\n",
    "        self_attn = self.self_attn(query=z, key=z, value=z, mask=mask)\n",
    "        # 将自注意力的输出加回到原始输入上\n",
    "        x = x + self.dropout(self_attn)\n",
    "\n",
    "        # 如果提供了源序列 (src)，则进行源注意力计算\n",
    "        if src is not None:\n",
    "            z = self.norm_src_attn(x)\n",
    "            attn_src = self.src_attn(query=z, key=src, value=src, mask=src_mask)\n",
    "            x = x + self.dropout(attn_src)\n",
    "\n",
    "        # 对注意力的输出进行层归一化后通过前馈网络\n",
    "        z = self.norm_ff(x)\n",
    "        ff = self.feed_forward(z)\n",
    "        x = x + self.dropout(ff)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../add_pic/transformer%20Encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer: TransformerLayer, n_layers: int):\n",
    "        super().__init__()\n",
    "        # 复制多个 Transformer 层\n",
    "        self.layers = clone_module_list(layer, n_layers)\n",
    "        # 对最终的输出进行层归一化\n",
    "        self.norm = nn.LayerNorm([layer.size])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        # 依次通过每个 Transformer 层\n",
    "        for layer in self.layers:\n",
    "            x = layer(x=x, mask=mask)\n",
    "        # 对最终的输出进行层归一化\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../add_pic/transformer%20Decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer: TransformerLayer, n_layers: int):\n",
    "        super().__init__()\n",
    "        # 复制多个 Transformer 层\n",
    "        self.layers = clone_module_list(layer, n_layers)\n",
    "        # 对最终的输出进行层归一化\n",
    "        self.norm = nn.LayerNorm([layer.size])\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                memory: torch.Tensor,\n",
    "                src_mask: torch.Tensor,\n",
    "                tgt_mask: torch.Tensor):\n",
    "        # 依次通过每个 Transformer 层\n",
    "        for layer in self.layers:\n",
    "            x = layer(x=x, mask=tgt_mask, src=memory, src_mask=src_mask)\n",
    "        # 对最终的输出进行层归一化\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_vocab: int, d_model: int):\n",
    "        super().__init__()\n",
    "        # 初始化线性层，用于将解码器的输出投影到词汇表大小的空间\n",
    "        self.projection = nn.Linear(d_model, n_vocab)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 应用线性层，将解码器的输出转换为词汇表上的分布\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: Encoder,\n",
    "                 decoder: Decoder,\n",
    "                 src_embed: nn.Module,\n",
    "                 tgt_embed: nn.Module,\n",
    "                 generator: nn.Module):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "        # 使用 Xavier 初始化参数\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self,\n",
    "                src: torch.Tensor,\n",
    "                tgt: torch.Tensor,\n",
    "                src_mask: torch.Tensor,\n",
    "                tgt_mask: torch.Tensor):\n",
    "        # 假设 src 和 tgt 的形状为 (batch_size, seq_len)\n",
    "        # 转换为 (seq_len, batch_size)\n",
    "        src = src.transpose(0, 1)\n",
    "        tgt = tgt.transpose(0, 1)\n",
    "\n",
    "        # 对 mask 进行相应转置\n",
    "        src_mask = src_mask.transpose(0, 2)  # 从 (batch_size, 1, seq_len) 变为 (seq_len, 1, batch_size)\n",
    "        tgt_mask = tgt_mask.transpose(0, 1).transpose(1, 2)  # 得到 (seq_len, seq_len, batch_size)\n",
    "\n",
    "        enc = self.encode(src, src_mask)\n",
    "        dec_output = self.decode(enc, src_mask, tgt, tgt_mask)\n",
    "        # 调用 Generator 层将 dec_output 投影到词汇表大小的维度\n",
    "        return self.generator(dec_output)\n",
    "\n",
    "\n",
    "    def encode(self, src: torch.Tensor, src_mask: torch.Tensor):\n",
    "        # 将源序列嵌入并传递给编码器\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self,\n",
    "               memory: torch.Tensor,\n",
    "               src_mask: torch.Tensor,\n",
    "               tgt: torch.Tensor,\n",
    "               tgt_mask: torch.Tensor):\n",
    "        # 将目标序列嵌入并传递给解码器，同时提供编码器的输出作为上下文\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 设置参数\n",
    "n_vocab = 100    # 假设词汇表大小为 100\n",
    "d_model = 512    # 编码/解码维度\n",
    "n_layers = 3     # Transformer层数\n",
    "heads = 8        # 多头注意力头数\n",
    "d_ff = 2048      # 前馈网络隐藏层维度\n",
    "dropout = 0.1    # Dropout 概率\n",
    "\n",
    "# 检查设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_data(seq_len, batch_size, n_vocab, dataset_size, device):\n",
    "    # 生成随机整数序列并放到 GPU（如果可用）\n",
    "    src = torch.randint(0, n_vocab, (dataset_size, seq_len), device=device)\n",
    "    tgt = torch.randint(0, n_vocab, (dataset_size, seq_len), device=device)\n",
    "\n",
    "    # 源序列掩码（形状 [batch_size, 1, seq_len]）\n",
    "    src_mask = torch.ones(dataset_size, 1, seq_len, device=device)\n",
    "    # 目标序列掩码（形状 [batch_size, seq_len, seq_len]），用于解码器防止看到未来位置\n",
    "    tgt_mask = (torch.tril(torch.ones(seq_len, seq_len, device=device)) == 1)\\\n",
    "                .unsqueeze(0).repeat(dataset_size, 1, 1)\n",
    "\n",
    "    return src, tgt, src_mask, tgt_mask\n",
    "\n",
    "# 数据相关参数\n",
    "seq_len = 10\n",
    "batch_size = 32\n",
    "dataset_size = 1000\n",
    "\n",
    "# 生成训练数据和测试数据\n",
    "src, tgt, src_mask, tgt_mask = generate_random_data(seq_len, batch_size, n_vocab, dataset_size, device)\n",
    "test_src, test_tgt, test_src_mask, test_tgt_mask = generate_random_data(seq_len, batch_size, n_vocab, dataset_size, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 DataLoader\n",
    "train_dataset = TensorDataset(src, tgt, src_mask, tgt_mask)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_src, test_tgt, test_src_mask, test_tgt_mask)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Encoder 层只需要自注意力和前馈网络\n",
    "encoder_self_attn = MultiHeadAttention(heads, d_model)\n",
    "encoder_feed_forward = FeedForward(d_model, d_ff)\n",
    "encoder_layer = TransformerLayer(d_model=d_model,\n",
    "                                 self_attn=encoder_self_attn,\n",
    "                                 feed_forward=encoder_feed_forward,\n",
    "                                 dropout_prob=dropout)\n",
    "\n",
    "# Decoder 层需要自注意力、cross attention 和前馈网络\n",
    "decoder_self_attn = MultiHeadAttention(heads, d_model)\n",
    "decoder_src_attn = MultiHeadAttention(heads, d_model)\n",
    "decoder_feed_forward = FeedForward(d_model, d_ff)\n",
    "decoder_layer = TransformerLayer(d_model=d_model,\n",
    "                                 self_attn=decoder_self_attn,\n",
    "                                 src_attn=decoder_src_attn,\n",
    "                                 feed_forward=decoder_feed_forward,\n",
    "                                 dropout_prob=dropout)\n",
    "\n",
    "\n",
    "# 构建编码器和解码器\n",
    "encoder = Encoder(encoder_layer, n_layers)\n",
    "decoder = Decoder(decoder_layer, n_layers)\n",
    "\n",
    "# 构建嵌入层和生成器\n",
    "src_embed = EmbeddingsWithPositionalEncoding(d_model, n_vocab)\n",
    "tgt_embed = EmbeddingsWithPositionalEncoding(d_model, n_vocab)\n",
    "generator = Generator(n_vocab, d_model)\n",
    "\n",
    "# 构建整个编码器-解码器模型\n",
    "model = EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)\n",
    "\n",
    "# 指定设备并移动模型到该设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 构建优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.859693542122841\n",
      "Epoch 2, Loss: 4.684530854225159\n",
      "Epoch 3, Loss: 4.6604111939668655\n",
      "Epoch 4, Loss: 4.657957315444946\n",
      "Epoch 5, Loss: 4.65576434135437\n",
      "Epoch 6, Loss: 4.643081501126289\n",
      "Epoch 7, Loss: 4.643091395497322\n",
      "Epoch 8, Loss: 4.640585109591484\n",
      "Epoch 9, Loss: 4.6413600742816925\n",
      "Epoch 10, Loss: 4.63554921746254\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for src, tgt, src_mask, tgt_mask in train_dataloader:\n",
    "        # 数据已经在 GPU 上（若 device=cuda），无需再次 .to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt, src_mask, tgt_mask)\n",
    "        # 使用交叉熵损失，忽略索引 0（可作为 <pad>）\n",
    "        loss = F.cross_entropy(output.view(-1, n_vocab), tgt.view(-1), ignore_index=0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.004882812500000001\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_accuracy = 0\n",
    "with torch.no_grad():\n",
    "    for src, tgt, src_mask, tgt_mask in test_dataloader:\n",
    "        output = model(src, tgt, src_mask, tgt_mask)\n",
    "        # 将 output 的维度从 (seq_len, batch_size) 转换为 (batch_size, seq_len)\n",
    "        output_max = output.argmax(dim=-1).transpose(0, 1)\n",
    "        correct = (output_max == tgt).sum().item()\n",
    "        total_accuracy += correct / (tgt.size(0) * tgt.size(1))\n",
    "\n",
    "print(f\"Accuracy: {total_accuracy/len(test_dataloader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
