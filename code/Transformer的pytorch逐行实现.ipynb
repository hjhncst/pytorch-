{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer模型结构\n",
    "![image.png](../add_pic/transformer结构.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 activation=nn.ReLU(),\n",
    "                 is_gated: bool = False,\n",
    "                 bias1: bool = True,\n",
    "                 bias2: bool = True,\n",
    "                 bias_gate: bool = True):\n",
    "        super().__init__()\n",
    "        # 初始化第一层线性变换，输入维度为 d_model，输出维度为 d_ff\n",
    "        self.layer1 = nn.Linear(d_model, d_ff, bias=bias1)\n",
    "        # 初始化第二层线性变换，输入维度为 d_ff，输出维度为 d_model\n",
    "        self.layer2 = nn.Linear(d_ff, d_model, bias=bias2)\n",
    "        # 初始化 dropout 层，用于在训练过程中随机关闭部分神经元\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 激活函数，用于引入非线性\n",
    "        self.activation = activation\n",
    "        # 是否启用门控机制\n",
    "        self.is_gated = is_gated\n",
    "        if is_gated:\n",
    "            # 如果启用门控机制，初始化另一层线性变换，用于门控\n",
    "            self.linear_v = nn.Linear(d_model, d_ff, bias=bias_gate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 应用第一层线性变换和激活函数\n",
    "        g = self.activation(self.layer1(x))\n",
    "        # 如果启用门控机制\n",
    "        if self.is_gated:\n",
    "            # 应用门控机制\n",
    "            g = g * self.linear_v(x)\n",
    "        else:\n",
    "            x = g\n",
    "        # 应用 dropout\n",
    "        x = self.dropout(g)\n",
    "        # 应用第二层线性变换\n",
    "        return self.layer2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../add_pic/transformer多头注意力机制1.png)\n",
    "![image.png](../add_pic/transformer多头注意力机制2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareForMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, d_k: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        # 初始化线性变换，用于将输入的 d_model 维度向量转换为 heads 个 d_k 维度向量\n",
    "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
    "        # 多头注意力的头数\n",
    "        self.heads = heads\n",
    "        # 单个头的维度\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x 的形状为 [seq_len, batch_size, d_model]\n",
    "        # 获取输入的初始形状，用于之后的变形操作\n",
    "        head_shape = x.shape[:-1]\n",
    "        # 将输入的 d_model 维度向量转换为 heads 个 d_k 维度向量\n",
    "        x = self.linear(x)\n",
    "        # 将最后一个维度分割为多个头，并为每个头分配 d_k 维度\n",
    "        x = x.view(*head_shape, self.heads, self.d_k)\n",
    "        # 返回转换后的张量，形状为 [seq_len, batch_size, heads, d_k]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, heads: int, d_k: int, dropout_prob: float = 0.1, bias: bool = True):\n",
    "        super().__init__()\n",
    "        # 计算每个注意力头的维度\n",
    "        self.d_k = d_model // heads\n",
    "        # 注意力头的数量\n",
    "        self.heads = heads\n",
    "\n",
    "        # 初始化query、key、value的线性变换\n",
    "        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias)\n",
    "        self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias)\n",
    "        self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias)\n",
    "\n",
    "        # 初始化 softmax 层\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # 输出层，将多个头的输出连接起来\n",
    "        self.output = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        # 初始化 dropout 层\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # 计算缩放因子\n",
    "        self.scale = 1 / math.sqrt(self.d_k)\n",
    "\n",
    "        # 存储注意力权重\n",
    "        self.attn = None\n",
    "\n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        # 计算query和key之间的点积\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "    def preper_mask(self, mask: torch.Tensor, query_shape: List[int], key_shape: List[int]):\n",
    "        # 调整 mask 的形状，使其与 query 和 key 的形状相匹配\n",
    "        if mask.dim() == 2:\n",
    "            mask = mask.unsqueeze(0)\n",
    "        if mask.size(1) == query_shape[0] and mask.size(2) == key_shape[0]:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        else:\n",
    "            raise ValueError(\"Mask shape is not compatible with query and key shape.\")\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, *, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        # 获取序列长度和批次大小\n",
    "        seq_len, batch_size = query.size(0), query.size(1)\n",
    "        \n",
    "        # 若提供了 mask，则调整 mask 的形状\n",
    "        if mask is not None:\n",
    "            mask = self.preper_mask(mask, query.shape, key.shape)\n",
    "\n",
    "        # 将 query、key 和 value 分别通过线性变换\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        # 计算注意力分数\n",
    "        scores = self.get_scores(query, key)\n",
    "\n",
    "        # 应用缩放因子\n",
    "        scores = scores * self.scale\n",
    "\n",
    "        # 应用 mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        attn = self.softmax(scores)\n",
    "\n",
    "        # 应用 dropout\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # 计算注意力输出\n",
    "        output = torch.matmul(attn, value)\n",
    "\n",
    "        # 存储注意力权重\n",
    "        self.attn = attn.detach()\n",
    "\n",
    "        # 将多个头的输出连接起来\n",
    "        output = output.view(seq_len, batch_size, -1)\n",
    "\n",
    "        # 通过输出层\n",
    "        output = self.output(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](../add_pic/transformer位置编码和嵌入.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout_prob: float, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # 初始化 dropout 层\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # 创建位置编码并注册为 buffer\n",
    "        self.register_buffer(\"position_encodings\", self.get_positional_encoding(d_model, max_len), False)\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            # 获取位置编码的一部分并确保不计算其梯度\n",
    "            pe = self.position_encodings[:x.shape[0]].detach().requires_grad_(False)\n",
    "            # 将位置编码添加到输入张量中\n",
    "            x = x + pe\n",
    "            # 应用 dropout\n",
    "            x = self.dropout(x)\n",
    "            return x\n",
    "    \n",
    "    def get_positional_encoding(self, d_model: int, max_len: int = 5000) -> torch.Tensor:\n",
    "        # 初始化一个全为 0 的位置编码矩阵，形状为 [max_len, d_model]\n",
    "        encodings = torch.zeros(max_len, d_model)\n",
    "        # 创建一个位置索引，形状为 [max_len, 1]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float().unsqueeze(0)\n",
    "        # 创建一个序列，用于在正弦和余弦函数中的分母，形状为 [d_model]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        tow_i = torch.arange(0, d_model, 2).float()\n",
    "        # 计算分母\n",
    "        div_term = torch.exp(tow_i * (-math.log(10000.0) / d_model))\n",
    "        # 填充位置编码的偶数索引\n",
    "        encodings[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 填充位置编码的奇数索引\n",
    "        encodings[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 增加一个批次维度\n",
    "        encodings = encodings.unsqueeze(1).requires_grad_(False)\n",
    "        return encodings\n",
    "\n",
    "    class EmbeddingsWithPositionalEncoding(nn.Module):\n",
    "        def __init__(self, d_model: int, n_vocab: int, max_len: int = 5000):\n",
    "            super().__init__()\n",
    "            # 初始化嵌入层，映射词汇表中的单词到 d_model 维度向量\n",
    "            self.embedding = nn.Embedding(n_vocab, d_model)\n",
    "            # 存储模型维度\n",
    "            self.d_model = d_model\n",
    "            # 生成并添加位置编码\n",
    "            self.register_buffer(\"position_encodings\", get_positional_encoding(d_model, max_len))\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            # 获取位置编码的一部分并确保不计算其梯度\n",
    "            pe = self.position_encodings[:x.shape[0]].detach().requires_grad_(False)\n",
    "            # 将嵌入向量乘以 d_model 的平方根并加上位置编码\n",
    "            return self.embedding(x) * math.sqrt(self.d_model) + pe\n",
    "        \n",
    "    class EmbeddingsWithLearnedEncoding(nn.Module):\n",
    "        def __init__(self, d_model: int, n_vocab: int, max_len: int = 5000):\n",
    "            super().__init__()\n",
    "            # 初始化嵌入层，映射词汇表中的单词到 d_model 维度向量\n",
    "            self.embedding = nn.Embedding(n_vocab, d_model)\n",
    "            # 存储模型维度\n",
    "            self.d_model = d_model\n",
    "            # 初始化位置编码\n",
    "            self.position_encodings = nn.Parameter(torch.zeros(max_len, 1, d_model), requires_grad=True)\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            # 获取位置编码\n",
    "            pe = self.position_encodings[:x.shape[0]]\n",
    "            # 将嵌入向量乘以 d_model 的平方根并加上位置编码\n",
    "            return self.embedding(x) * math.sqrt(self.d_model) + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
